from google import genai
from google.genai import types
import json
from pathlib import Path
from typing import Callable, List, Dict, Any
import asyncio
import threading

from src.utils.config import Config
from src.core.tools import Tool
from src.interfaces.llm_backend import ILLMBackend

class GoogleAIBackend(ILLMBackend):
    def __init__(self):
        llm_config = Config.data.get("llm", {})
        self.api_key = llm_config.get("api_key")
        self.model_name = llm_config.get("model")

        # Load System Instructions
        # e:/_Projects/Miyori/src/implementations/llm/google_ai_backend.py
        project_root = Path(__file__).parent.parent.parent.parent
        system_instruction_file = llm_config.get("system_instruction_file", "system_instructions.txt")
        self.system_instruction_path = project_root / system_instruction_file
        self.system_instruction = None

        if self.system_instruction_path.exists():
            try:
                with open(self.system_instruction_path, "r", encoding="utf-8") as f:
                    self.system_instruction = f.read().strip()
            except Exception as e:
                print(f"Error reading system instruction file: {e}")
        else:
            print(f"Warning: System instruction file not found at {self.system_instruction_path}")
        
        if self.api_key:
            self.client = genai.Client(api_key=self.api_key)
        else:
            # Handle missing API key gracefully or let it fail later
            self.client = None
            
        self.chat_session = None
        
        # Memory Components
        try:
            from src.memory.sqlite_store import SQLiteMemoryStore
            from src.utils.embeddings import EmbeddingService
            from src.memory.episodic import EpisodicMemoryManager
            from src.memory.summarizer import Summarizer
            from src.memory.context import ContextBuilder
            from src.memory.gates import MemoryGate
            from src.memory.memory_retriever import MemoryRetriever
            from src.memory.async_memory_stream import AsyncMemoryStream

            self.store = SQLiteMemoryStore()
            self.embedding_service = EmbeddingService()
            self.episodic_manager = EpisodicMemoryManager(self.store, self.embedding_service)
            self.summarizer = Summarizer(self.client)

            # Dual-mode memory components
            self.memory_retriever = MemoryRetriever(self.store)
            self.async_memory_stream = AsyncMemoryStream(
                self.memory_retriever,
                self.embedding_service
            )

            self.context_builder = ContextBuilder(
                self.store,
                self.episodic_manager,
                self.async_memory_stream
            )
            self.gate = MemoryGate(self.client)
            
            # Phase 3 Components
            from src.memory.consolidation import ConsolidationManager, SemanticExtractor
            
            self.semantic_extractor = SemanticExtractor(self.client, self.store)
            self.consolidation = ConsolidationManager(
                self.store, self.episodic_manager, 
                self.semantic_extractor
            )
            
            memory_config = Config.data.get("memory", {})
            self.memory_enabled = memory_config.get("enabled", True)
            self.feature_flags = memory_config.get("feature_flags", {})

            # Async Background Loop for memory tasks
            self._loop = asyncio.new_event_loop()
            self._loop_thread = threading.Thread(target=self._loop.run_forever, daemon=True)
            self._loop_thread.start()

            # Start async memory streaming
            if self.memory_enabled:
                self._run_async(self.async_memory_stream.start())
                print("Memory system initialized and enabled with dual-mode retrieval.")
        except Exception as e:
            print(f"Warning: Failed to initialize memory system: {e}")
            self.memory_enabled = False

    def _run_async(self, coro):
        """Run a coroutine in the background thread's event loop."""
        if hasattr(self, '_loop'):
            return asyncio.run_coroutine_threadsafe(coro, self._loop)
        return None

    async def _store_turn(self, user_msg: str, miyori_msg: str):
        """Summarize and store the conversation turn."""
        try:
            # Phase 2: Memory Gating
            if self.feature_flags.get("enable_gating", False):
                if not await self.gate.should_remember(user_msg, miyori_msg):
                    print("Memory: Turn skipped by Gate.")
                    return

            print(f"Memory: Summarizing turn...")
            summary = await self.summarizer.create_summary(user_msg, miyori_msg)
            full_text = {"user": user_msg, "miyori": miyori_msg}
            
            print(f"Memory: Storing episode...")
            await self.episodic_manager.add_episode(summary, full_text)
            print(f"Memory: Episode stored successfully.")

            # Add to async memory stream for next turn prefetching
            if hasattr(self, 'async_memory_stream'):
                self.async_memory_stream.add_turn_context(user_msg, miyori_msg)
            
        except Exception as e:
            import sys
            sys.stderr.write(f"Memory Error: Failure in _store_turn: {e}\n")

    def reset_context(self) -> None:
        """Resets the conversation history."""
        print("Resetting conversation context...")
        self.chat_session = None
        # Clear async memory stream context
        if hasattr(self, 'async_memory_stream'):
            self.async_memory_stream._recent_turns.clear()

    async def _cleanup_async_memory(self):
        """Cleanup async memory stream."""
        if hasattr(self, 'async_memory_stream'):
            await self.async_memory_stream.stop()

    def llm_chat(
        self,
        prompt: str,
        tools: List[Tool],
        on_chunk: Callable[[str], None],
        on_tool_call: Callable[[str, Dict[str, Any]], str]
    ) -> None:
        if not self.client:
            print("Error: API Key not configured.")
            return

        # Convert tools to Gemini format
        google_tools = self._convert_tools_to_gemini_format(tools)
        
        # Initialize chat if not already active
        if self.chat_session is None:
            # Fetch Context
            context = ""
            if self.memory_enabled:
                try:
                    context = self.context_builder.build_context(prompt)
                except Exception as e:
                    print(f"Memory retrieval failed: {e}")

            try:
                effective_system_instruction = self.system_instruction or ""
                if context:
                    effective_system_instruction += f"\n\n[PAST CONTEXT]\n{context}\n"

                config = types.GenerateContentConfig(
                    system_instruction=effective_system_instruction if effective_system_instruction else None,
                    tools=google_tools if google_tools else None
                )
                self.chat_session = self.client.chats.create(model=self.model_name, config=config)
            except Exception as e:
                print(f"Error creating chat session: {e}")
                return
        else:
            # If chat exists, we might need to update the tools in the config for this session
            # However, google-genai chat sessions usually keep their config.
            # For simplicity, if tools change, we might need a new chat or update config.
            # In Miyori, tools are usually static after startup.
            pass

        max_turns = 10
        turn_count = 0
        full_response = []

        try:
            # First turn: Send user prompt
            response = self.chat_session.send_message(prompt)
            
            while turn_count < max_turns:
                turn_count += 1
                has_tool_call = False
                tool_response_parts = []
                
                # Process parts of the current response
                if response.candidates and response.candidates[0].content.parts:
                    for part in response.candidates[0].content.parts:
                        if part.text:
                            full_response.append(part.text)
                            on_chunk(part.text)
                        
                        if part.function_call:
                            has_tool_call = True
                            tool_name = part.function_call.name
                            args = part.function_call.args or {}
                            
                            # Execute tool
                            result = on_tool_call(tool_name, args)
                            
                            # Prepare result part
                            tool_response_parts.append(types.Part.from_function_response(
                                name=tool_name,
                                response={"result": result}
                            ))
                
                if has_tool_call:
                    # Send all tool results back to the model in one go
                    # This triggers the next turn
                    response = self.chat_session.send_message(tool_response_parts)
                else:
                    # No more tool calls in this response, we are finished
                    if self.memory_enabled:
                        self._run_async(self._store_turn(prompt, "".join(full_response)))
                    break

            if turn_count >= max_turns:
                print(f"⚠️ Warning: Max tool turns ({max_turns}) reached.")

        except Exception as e:
            print(f"Error during tool-enabled generation: {e}")
            self.chat_session = None

    def _convert_tools_to_gemini_format(self, tools: List[Tool]) -> List[types.Tool]:
        if not tools:
            return []
        
        function_declarations = []
        for tool in tools:
            properties = {}
            required = []
            
            for param in tool.parameters:
                # Map our types to Gemini Schema types
                # Gemini expects uppercase strings or types.Type enum
                param_type = param.type.upper()
                if param_type == "NUMBER": param_type = "NUMBER"
                elif param_type == "INTEGER": param_type = "INTEGER"
                elif param_type == "BOOLEAN": param_type = "BOOLEAN"
                elif param_type == "ARRAY": param_type = "ARRAY"
                elif param_type == "OBJECT": param_type = "OBJECT"
                else: param_type = "STRING"

                prop = {
                    "type": param_type,
                    "description": param.description
                }
                if param.enum:
                    prop["enum"] = param.enum
                
                properties[param.name] = prop
                if param.required:
                    required.append(param.name)
            
            function_declarations.append(
                types.FunctionDeclaration(
                    name=tool.name,
                    description=tool.description,
                    parameters=types.Schema(
                        type="OBJECT",
                        properties=properties,
                        required=required
                    )
                )
            )
            
        return [types.Tool(function_declarations=function_declarations)]
