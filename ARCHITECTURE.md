# ðŸ›‘ STATUS: COMPLETED (Phase 1)
> This document outlines the initial implementation which is now live. 
> Do not use this as a task list. Treat it as historical context or architecture reference only.
# Miyori - Phase 1 Architecture

## Project Structure
```
miyori/
â”œâ”€â”€ main.py
â”œâ”€â”€ config.json
â”œâ”€â”€ config.json.example          # Template config file
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ AGENTS.md                    # AI agent instructions (minimal)
â”œâ”€â”€ ARCHITECTURE.md              # This file
â””â”€â”€ src/
    â”œâ”€â”€ interfaces/
    â”‚   â”œâ”€â”€ speech_input.py      # ISpeechInput interface
    â”‚   â”œâ”€â”€ speech_output.py     # ISpeechOutput interface
    â”‚   â””â”€â”€ llm_backend.py       # ILLMBackend interface
    â”œâ”€â”€ implementations/
    â”‚   â”œâ”€â”€ speech/
    â”‚   â”‚   â”œâ”€â”€ SPEECH_INPUT_PLAN.md     # How to implement speech input
    â”‚   â”‚   â””â”€â”€ google_speech_input.py
    â”‚   â”œâ”€â”€ tts/
    â”‚   â”‚   â”œâ”€â”€ TTS_OUTPUT_PLAN.md       # How to implement TTS
    â”‚   â”‚   â””â”€â”€ pyttsx_output.py
    â”‚   â””â”€â”€ llm/
    â”‚       â”œâ”€â”€ LLM_BACKEND_PLAN.md      # How to implement LLM
    â”‚       â””â”€â”€ google_ai_backend.py
    â””â”€â”€ core/
        â”œâ”€â”€ ASSISTANT_PLAN.md            # How to implement assistant
        â””â”€â”€ assistant.py
```

## Core Interfaces

### ISpeechInput
```python
def listen(self) -> str | None:
    """Listen and return transcribed text"""
```

### ISpeechOutput
```python
def speak(self, text: str) -> None:
    """Convert text to speech"""
```

### ILLMBackend
```python
def generate(self, prompt: str) -> str:
    """Generate AI response"""
```

## Design Rules

1. **All implementations read from config.json** (in root)
2. **No error handling this phase unless explicitly stated**
3. **Use print() for output** (no logging framework)
4. **Full type hints everywhere**
5. **Each implementation folder contains a file ending in _PLAN.md with specific instructions**
6. **main.py just wires components together**

## Implementation Order

1. **Interfaces are already defined** in `src/interfaces/` (speech_input.py, speech_output.py, llm_backend.py)
2. Copy `config.json.example` to `config.json` and add your Google AI API key
3. Implement GoogleSpeechInput (see `src/implementations/speech/SPEECH_INPUT_PLAN.md`)
4. Implement PyttsxOutput (see `src/implementations/tts/TTS_OUTPUT_PLAN.md`)
5. Implement GoogleAIBackend (see `src/implementations/llm/LLM_BACKEND_PLAN.md`)
6. Build VoiceAssistant (see `src/core/ASSISTANT_PLAN.md`)
7. Wire in main.py

## Phase 1 Goal
Speak â†’ AI responds â†’ Hear response (looping)